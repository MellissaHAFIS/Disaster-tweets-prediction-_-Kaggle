{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddd6a6b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:45.184221Z",
     "iopub.status.busy": "2024-10-04T09:02:45.183646Z",
     "iopub.status.idle": "2024-10-04T09:02:46.148529Z",
     "shell.execute_reply": "2024-10-04T09:02:46.147273Z"
    },
    "papermill": {
     "duration": 0.984253,
     "end_time": "2024-10-04T09:02:46.151584",
     "exception": false,
     "start_time": "2024-10-04T09:02:45.167331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/sample-submission/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280c944a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:46.180565Z",
     "iopub.status.busy": "2024-10-04T09:02:46.179904Z",
     "iopub.status.idle": "2024-10-04T09:02:47.929985Z",
     "shell.execute_reply": "2024-10-04T09:02:47.928833Z"
    },
    "papermill": {
     "duration": 1.76768,
     "end_time": "2024-10-04T09:02:47.932840",
     "exception": false,
     "start_time": "2024-10-04T09:02:46.165160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47f029",
   "metadata": {
    "papermill": {
     "duration": 0.013945,
     "end_time": "2024-10-04T09:02:47.961131",
     "exception": false,
     "start_time": "2024-10-04T09:02:47.947186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview of what we will do: \n",
    "\n",
    "1. **Data preprocessing**: Cleaning the tweets by removing irrelevant elements like hashtags, mentions, and URLs.\n",
    "2. **Tokenization**: Breaking down the tweets into individual words or tokens.\n",
    "3. **Vectorization**: Converting the text into numerical data (using techniques like Bag of Words or TF-IDF).\n",
    "4. **Model building**: Using machine learning models (e.g., logistic regression, Naive Bayes) to classify the tweets as related to disasters or not.\n",
    "5. **Evaluation**: Assessing model performance using accuracy, precision, recall, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709df95",
   "metadata": {
    "papermill": {
     "duration": 0.012883,
     "end_time": "2024-10-04T09:02:47.987140",
     "exception": false,
     "start_time": "2024-10-04T09:02:47.974257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why did i choose those libraries? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a936c9",
   "metadata": {
    "papermill": {
     "duration": 0.012791,
     "end_time": "2024-10-04T09:02:48.013148",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.000357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **`numpy`**: Great for numerical operations, though for most tasks in NLP, we might not need it much beyond array manipulations.\n",
    "- **`pandas`**: Essential for handling and preprocessing your dataset (like loading CSV files, handling missing data, etc.).\n",
    "- **`sklearn.feature_extraction`**: we will likely use this for text vectorization, like **TF-IDF** or **CountVectorizer** (we will use in this project CountVectorizer), which is perfect for converting your tweet data into a format (numercial format) suitable for machine learning models.\n",
    "- **`sklearn.linear_model`**: Good choice for basic classifiers like **Logistic Regression**, which is commonly used for binary classification tasks (disaster vs. non-disaster).\n",
    "- **`sklearn.model_selection`**: This will help us with tasks like **train/test splitting** and **cross-validation** to evaluate your model's performance.\n",
    "- **`sklearn.preprocessing`**: Useful for scaling, encoding, or transforming data, although we might not need much preprocessing beyond text vectorization in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8278b",
   "metadata": {
    "papermill": {
     "duration": 0.012864,
     "end_time": "2024-10-04T09:02:48.039216",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.026352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is the difference between TF-IDF and CountVectorizer??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fc86b",
   "metadata": {
    "papermill": {
     "duration": 0.014192,
     "end_time": "2024-10-04T09:02:48.066610",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.052418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Both **CountVectorizer** and **TF-IDF** are techniques used to convert text data into numerical features, but they do it in slightly different ways:\n",
    "\n",
    "### 1. **CountVectorizer**:\n",
    "- **What it does**: It converts text into a matrix of token counts.\n",
    "- **How it works**: Each document (in your case, each tweet) is represented as a row in a matrix, and each word (or token) in the entire dataset is represented as a column. The value in each cell is the count of how many times the word appears in that tweet.\n",
    "- **Example**: \n",
    "  - Tweet 1: \"Disaster struck city\"\n",
    "  - Tweet 2: \"Disaster response team\"\n",
    "  \n",
    "  The vocabulary might be: `[\"Disaster\", \"struck\", \"city\", \"response\", \"team\"]`. The matrix will look like this:\n",
    "  \n",
    "  ```\n",
    "  Tweet 1: [1, 1, 1, 0, 0]\n",
    "  Tweet 2: [1, 0, 0, 1, 1]\n",
    "  ```\n",
    "\n",
    "- **Pros**: Simple and easy to implement.\n",
    "- **Cons**: It treats all words equally, so common words (like \"the,\" \"and\") can dominate the matrix, and it doesn't account for word importance.\n",
    "\n",
    "### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "- **What it does**: It converts text into a matrix that reflects how important each word is, both in individual documents and across the whole dataset.\n",
    "- **How it works**:\n",
    "  - **TF (Term Frequency)**: The number of times a word appears in a tweet, just like CountVectorizer.\n",
    "  - **IDF (Inverse Document Frequency)**: A measure of how rare a word is across all tweets. Words that appear frequently in many tweets (like \"disaster\") get a lower score because they're less informative.\n",
    "  \n",
    "  The final score for each word is computed as:\n",
    "  \\[\n",
    "  \\text{TF-IDF} = \\text{TF} \\times \\text{IDF}\n",
    "  \\]\n",
    "  \n",
    "  So, words that appear frequently in one tweet but rarely in others get higher importance, while common words get lower importance.\n",
    "\n",
    "- **Example**:\n",
    "  - In the above example, \"Disaster\" appears in both tweets, so its importance would be reduced in TF-IDF, while words like \"struck\" and \"response\" may get higher weights since they are less frequent.\n",
    "\n",
    "- **Pros**: It downweights common words and highlights rare but important words.\n",
    "- **Cons**: Slightly more complex than CountVectorizer and may require more tuning.\n",
    "\n",
    "### When to Use Which?\n",
    "- **CountVectorizer**: Use it when you want a simple and straightforward approach, especially for models that can handle high-dimensional sparse data, like Naive Bayes.\n",
    "- **TF-IDF**: Use it when you want to capture the importance of words and avoid over-representing common words in your dataset, especially in classification tasks where the meaning of words (not just their presence) matters.\n",
    "\n",
    "For your disaster tweet classification task, **TF-IDF** might be a better choice because it helps to highlight the less common but more informative words that are related to disasters. Would you like to try implementing TF-IDF first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06048d",
   "metadata": {
    "papermill": {
     "duration": 0.012865,
     "end_time": "2024-10-04T09:02:48.092597",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.079732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f1df61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.121135Z",
     "iopub.status.busy": "2024-10-04T09:02:48.120465Z",
     "iopub.status.idle": "2024-10-04T09:02:48.202243Z",
     "shell.execute_reply": "2024-10-04T09:02:48.200889Z"
    },
    "papermill": {
     "duration": 0.099645,
     "end_time": "2024-10-04T09:02:48.205313",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.105668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_train_df=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "full_test_df=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5ea74",
   "metadata": {
    "papermill": {
     "duration": 0.013287,
     "end_time": "2024-10-04T09:02:48.232589",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.219302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Data exploring and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1a861d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.261306Z",
     "iopub.status.busy": "2024-10-04T09:02:48.260821Z",
     "iopub.status.idle": "2024-10-04T09:02:48.305392Z",
     "shell.execute_reply": "2024-10-04T09:02:48.304037Z"
    },
    "papermill": {
     "duration": 0.062466,
     "end_time": "2024-10-04T09:02:48.308381",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.245915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first exampls of the data frame\n",
      "    id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "\n",
      "The last exampls of the data frame\n",
      "          id keyword location  \\\n",
      "7608  10869     NaN      NaN   \n",
      "7609  10870     NaN      NaN   \n",
      "7610  10871     NaN      NaN   \n",
      "7611  10872     NaN      NaN   \n",
      "7612  10873     NaN      NaN   \n",
      "\n",
      "                                                   text  target  \n",
      "7608  Two giant cranes holding a bridge collapse int...       1  \n",
      "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
      "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
      "7611  Police investigating after an e-bike collided ...       1  \n",
      "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
      "\n",
      "Columns of our full data frame\n",
      " Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "\n",
      "five real disaster tweets: \n",
      " ['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
      " 'Forest fire near La Ronge Sask. Canada'\n",
      " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      " '13,000 people receive #wildfires evacuation orders in California '\n",
      " 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school ']\n",
      "\n",
      "five not real disaster tweets: \n",
      " [\"What's up man?\" 'I love fruits' 'Summer is lovely' 'My car is so fast'\n",
      " 'What a goooooooaaaaaal!!!!!!']\n",
      "Statistics of our full data fram                  id      target\n",
      "count   7613.000000  7613.00000\n",
      "mean    5441.934848     0.42966\n",
      "std     3137.116090     0.49506\n",
      "min        1.000000     0.00000\n",
      "25%     2734.000000     0.00000\n",
      "50%     5408.000000     0.00000\n",
      "75%     8146.000000     1.00000\n",
      "max    10873.000000     1.00000\n",
      "The number of real disters is  3271\n",
      "The number of not real disaters is  4342\n"
     ]
    }
   ],
   "source": [
    "print(\"The first exampls of the data frame\\n\",full_train_df.head())\n",
    "print(\"\\nThe last exampls of the data frame\\n\", full_train_df.tail())\n",
    "print(\"\\nColumns of our full data frame\\n\", full_train_df.columns)\n",
    "#the real disaster tweets\n",
    "print(\"\\nfive real disaster tweets: \\n\", full_train_df[full_train_df['target']==1]['text'].values[0:5])\n",
    "print(\"\\nfive not real disaster tweets: \\n\", full_train_df[full_train_df['target']==0]['text'].values[0:5])\n",
    "\n",
    "print(\"Statistics of our full data fram\", full_train_df.describe())\n",
    "\n",
    "\n",
    "#the number of real disaster and not real (important information to know if we need to balance the model of not )\n",
    "nb_real_dis=full_train_df[full_train_df['target']==1]['id'].count()\n",
    "nb_Notreal_dis=full_train_df[full_train_df['target']==0]['id'].count()\n",
    "print(\"The number of real disters is \",nb_real_dis )\n",
    "print(\"The number of not real disaters is \", nb_Notreal_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9b44c",
   "metadata": {
    "papermill": {
     "duration": 0.013407,
     "end_time": "2024-10-04T09:02:48.335143",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.321736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*i think for the feature ingeeniring we will use the columns: keyword, location, text, target*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757c19f",
   "metadata": {
    "papermill": {
     "duration": 0.01303,
     "end_time": "2024-10-04T09:02:48.361526",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.348496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Obtain target and predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d422e380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.390373Z",
     "iopub.status.busy": "2024-10-04T09:02:48.389907Z",
     "iopub.status.idle": "2024-10-04T09:02:48.396025Z",
     "shell.execute_reply": "2024-10-04T09:02:48.394869Z"
    },
    "papermill": {
     "duration": 0.024056,
     "end_time": "2024-10-04T09:02:48.398914",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.374858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(full_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01e9e5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.427866Z",
     "iopub.status.busy": "2024-10-04T09:02:48.427417Z",
     "iopub.status.idle": "2024-10-04T09:02:48.497337Z",
     "shell.execute_reply": "2024-10-04T09:02:48.496062Z"
    },
    "papermill": {
     "duration": 0.087677,
     "end_time": "2024-10-04T09:02:48.500028",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.412351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.head() is : \n",
      "   keyword location                                               text\n",
      "0     NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
      "1     NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
      "2     NaN      NaN  All residents asked to 'shelter in place' are ...\n",
      "3     NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
      "4     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...\n",
      "Statistical description of the train data frame: \n",
      "            keyword location                                               text\n",
      "count         7552     5080                                               7613\n",
      "unique         221     3341                                               7503\n",
      "top     fatalities      USA  11-Year-Old Boy Charged With Manslaughter of T...\n",
      "freq            45      104                                                 10\n",
      "test_df.head() is : \n",
      "   keyword location                                               text\n",
      "0     NaN      NaN                 Just happened a terrible car crash\n",
      "1     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
      "Statistical description of the test data frame: \n",
      "         keyword  location                                               text\n",
      "count      3237      2158                                               3263\n",
      "unique      221      1602                                               3243\n",
      "top     deluged  New York  11-Year-Old Boy Charged With Manslaughter of T...\n",
      "freq         23        38                                                  3\n",
      "let's see if there is have more details about the messing values in the data frames train_df and test_df: \n",
      "\n",
      "  ->cols with missing values in train_df ['keyword', 'location']\n",
      "  ->The number of missing values in each column above in the train_df is:\n",
      " keyword       61\n",
      "location    2533\n",
      "dtype: int64\n",
      "  ->cols with missing values in test_df ['keyword', 'location']\n",
      "  ->The number of missing values in each column above in the test_df is:\n",
      " keyword       26\n",
      "location    1105\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Y=full_train_df.target\n",
    "features=[\"keyword\", \"location\", \"text\"]\n",
    "train_df=pd.DataFrame(full_train_df[features])\n",
    "test_df=pd.DataFrame(full_test_df[features])\n",
    "#NOTE: FOR MODIFICATION IT'S NOT POSSIBLE TO MODIFY ON A COPY SO IT'S BETTER TO CREATE A NEW DATAFRAME\n",
    "#train_df=full_train_df[features] \n",
    "#test_df=full_test_df[features]\n",
    "\n",
    "print(\"train_df.head() is : \\n\",train_df.head())\n",
    "print(\"Statistical description of the train data frame: \\n\", train_df.describe())\n",
    "\n",
    "print(\"test_df.head() is : \\n\",test_df.head())\n",
    "print(\"Statistical description of the test data frame: \\n\", test_df.describe())\n",
    "#from the privious prints we will see some of the columns in train_df and test_df contain missing values\n",
    "print(\"let's see if there is have more details about the messing values in the data frames train_df and test_df: \\n\")\n",
    "\n",
    "cols_with_missing_train =[col for col in train_df.columns if train_df[col].isnull().any()] \n",
    "print(\"  ->cols with missing values in train_df\", cols_with_missing_train )\n",
    "nb_missing_values_per_col_train=train_df[cols_with_missing_train].isnull().sum()\n",
    "print(\"  ->The number of missing values in each column above in the train_df is:\\n\", nb_missing_values_per_col_train)\n",
    "\n",
    "cols_with_missing_test =[col for col in test_df.columns if test_df[col].isnull().any()] \n",
    "print(\"  ->cols with missing values in test_df\", cols_with_missing_test )\n",
    "nb_missing_values_per_col_test=test_df[cols_with_missing_test].isnull().sum()\n",
    "print(\"  ->The number of missing values in each column above in the test_df is:\\n\", nb_missing_values_per_col_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6c6eb",
   "metadata": {
    "papermill": {
     "duration": 0.013398,
     "end_time": "2024-10-04T09:02:48.527311",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.513913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Handling missing values\n",
    "\n",
    "As we have seen in privious analysis we have many null values.\n",
    "\n",
    "To handle the missing values in our data frame we will use the methode of Extanded imputation: \n",
    "An Extension To Imputation: In this approach, we impute the missing values (ie replace theme with something else like a mean, or an other text..etc). And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.\n",
    "So also here we use the same process as in the previous, but we add some steps: adding the columns indicating where the imputed values.\n",
    "\n",
    "**BUT**, to use the methode of imputer, using the SimpleImputer, the columns must be numerical however in our data frames the columns having missing values have string type.\n",
    "\n",
    "So, we are going to follow the same process but instead of using the simpleImputer (that replaces Null with the mean) we will do it manually and we will replace the null values by a \"unkown_location\", \"unknowen_keyword\" for missing location and missing keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5c30c",
   "metadata": {
    "papermill": {
     "duration": 0.013286,
     "end_time": "2024-10-04T09:02:48.554793",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.541507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "**NOTE**: **SimpleImputer** is a class in the scikit-learn library that provides basic strategies for imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232bf1ec",
   "metadata": {
    "papermill": {
     "duration": 0.013331,
     "end_time": "2024-10-04T09:02:48.581694",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.568363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **A mistake: **\n",
    "Using SimpleImputer with String Columns\n",
    "You can still use SimpleImputer with string columns if you specify the strategy parameter correctly. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b98e89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.611723Z",
     "iopub.status.busy": "2024-10-04T09:02:48.611290Z",
     "iopub.status.idle": "2024-10-04T09:02:48.616740Z",
     "shell.execute_reply": "2024-10-04T09:02:48.615604Z"
    },
    "papermill": {
     "duration": 0.023327,
     "end_time": "2024-10-04T09:02:48.619144",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.595817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#do not run the cell\n",
    "######################################### remarque ###################################################\n",
    "#from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create the imputer with a constant fill value\n",
    "#imputer = SimpleImputer(strategy='constant', fill_value='no_keyword')\n",
    "\n",
    "# Apply imputer to the 'keyword' column (ensure it's 2D by using double brackets)\n",
    "#train_df['keyword'] = imputer.fit_transform(train_df[['keyword']])\n",
    "#test_df['keyword'] = imputer.transform(test_df[['keyword']])\n",
    "\n",
    "# For the 'location' column\n",
    "#imputer = SimpleImputer(strategy='constant', fill_value='no_location')\n",
    "#train_df['location'] = imputer.fit_transform(train_df[['location']])\n",
    "#test_df['location'] = imputer.transform(test_df[['location']])\n",
    "\n",
    "########################################## fin de la remarque ########################################\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d860ffa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.649002Z",
     "iopub.status.busy": "2024-10-04T09:02:48.648575Z",
     "iopub.status.idle": "2024-10-04T09:02:48.666324Z",
     "shell.execute_reply": "2024-10-04T09:02:48.665239Z"
    },
    "papermill": {
     "duration": 0.036,
     "end_time": "2024-10-04T09:02:48.668846",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.632846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#befor imputing the null values and adding new columns\n",
    "#we should do a copy of our original data frames\n",
    "#i am not going to do it because i already have full_train_df and full_test_df\n",
    "\n",
    "#we will add columns that indicate if there is a missing value or not \n",
    "for col in cols_with_missing_train: \n",
    "    train_df[col+'_was_missing']=train_df[col].isnull()\n",
    "    test_df[col+'_was_missing']=test_df[col].isnull()\n",
    "#impoutation \n",
    "train_df['keyword'] = train_df['keyword'].fillna('no_keyword')\n",
    "train_df['location'] = train_df['location'].fillna('no_location')\n",
    "\n",
    "test_df['keyword'] = test_df['keyword'].fillna('no_keyword')\n",
    "test_df['location'] = test_df['location'].fillna('no_location')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47cddfa",
   "metadata": {
    "papermill": {
     "duration": 0.01348,
     "end_time": "2024-10-04T09:02:48.696080",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.682600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "let's check the null values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36b8cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.725994Z",
     "iopub.status.busy": "2024-10-04T09:02:48.724925Z",
     "iopub.status.idle": "2024-10-04T09:02:48.741980Z",
     "shell.execute_reply": "2024-10-04T09:02:48.740761Z"
    },
    "papermill": {
     "duration": 0.034684,
     "end_time": "2024-10-04T09:02:48.744467",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.709783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ->cols with missing values in train_df []\n",
      "  ->The number of missing values in each column above in the train_df is:\n",
      " Series([], dtype: float64)\n",
      "  ->cols with missing values in test_df []\n",
      "  ->The number of missing values in each column above in the test_df is:\n",
      " Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cols_with_missing_train =[col for col in train_df.columns if train_df[col].isnull().any()] \n",
    "print(\"  ->cols with missing values in train_df\", cols_with_missing_train )\n",
    "nb_missing_values_per_col_train=train_df[cols_with_missing_train].isnull().sum()\n",
    "print(\"  ->The number of missing values in each column above in the train_df is:\\n\", nb_missing_values_per_col_train)\n",
    "\n",
    "cols_with_missing_test =[col for col in test_df.columns if test_df[col].isnull().any()] \n",
    "print(\"  ->cols with missing values in test_df\", cols_with_missing_test )\n",
    "nb_missing_values_per_col_test=test_df[cols_with_missing_test].isnull().sum()\n",
    "print(\"  ->The number of missing values in each column above in the test_df is:\\n\", nb_missing_values_per_col_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb521e",
   "metadata": {
    "papermill": {
     "duration": 0.013481,
     "end_time": "2024-10-04T09:02:48.771737",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.758256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "let's verify if the new columns a well added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f93f3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.801345Z",
     "iopub.status.busy": "2024-10-04T09:02:48.800861Z",
     "iopub.status.idle": "2024-10-04T09:02:48.809774Z",
     "shell.execute_reply": "2024-10-04T09:02:48.808660Z"
    },
    "papermill": {
     "duration": 0.026578,
     "end_time": "2024-10-04T09:02:48.812135",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.785557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['keyword', 'location', 'text', 'keyword_was_missing',\n",
       "       'location_was_missing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns\n",
    "test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44564f",
   "metadata": {
    "papermill": {
     "duration": 0.014675,
     "end_time": "2024-10-04T09:02:48.840875",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.826200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Data preprocessing: Cleaning the tweets by removing irrelevant elements like hashtags, mentions, and URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e133c1c",
   "metadata": {
    "papermill": {
     "duration": 0.013964,
     "end_time": "2024-10-04T09:02:48.869271",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.855307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Problem: Tweets often contain special characters, hashtags, mentions, and URLs that can affect the model.\n",
    "\n",
    "Suggestions: Clean up the text column to remove unnecessary elements like URLs, mentions (@username), hashtags, and convert everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6718190b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:48.899429Z",
     "iopub.status.busy": "2024-10-04T09:02:48.898922Z",
     "iopub.status.idle": "2024-10-04T09:02:49.012929Z",
     "shell.execute_reply": "2024-10-04T09:02:49.011459Z"
    },
    "papermill": {
     "duration": 0.133259,
     "end_time": "2024-10-04T09:02:49.016603",
     "exception": false,
     "start_time": "2024-10-04T09:02:48.883344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb3e10",
   "metadata": {
    "papermill": {
     "duration": 0.017837,
     "end_time": "2024-10-04T09:02:49.050163",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.032326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Building vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e55e35",
   "metadata": {
    "papermill": {
     "duration": 0.019471,
     "end_time": "2024-10-04T09:02:49.086185",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.066714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's dive deeper into the two parts: vectorizing using TF-IDF and CountVectorizer and modeling with RidgeClassifier.\n",
    "\n",
    "*# **1. Vectorizing with TF-IDF and CountVectorizer**\n",
    "*Both TF-IDF (Term Frequency - Inverse Document Frequency) and CountVectorizer are ways to convert text data into a numerical format that can be used by machine learning models.\n",
    "\n",
    "* ****CountVectorizer:\n",
    "What it does: It simply counts the number of times each word (or token) appears in each document (in your case, each tweet).\n",
    "\n",
    "Result: You get a term-document matrix where each row represents a tweet, and each column represents a word from your entire vocabulary.\n",
    "\n",
    "* ****TF-IDF:\n",
    "What it does: It combines term frequency (how often a word appears in a tweet) with inverse document frequency (how unique or rare that word is across the entire dataset). Words that are very common across all tweets (e.g., “the,” “and”) will have a low score, while rarer and more significant words will have a higher score.\n",
    "\n",
    "Result: A weighted term-document matrix where the value represents not just how often the word appears, but also how important the word is in the context of the dataset.\n",
    "\n",
    "Which one to use?\n",
    "CountVectorizer: Simple and fast. If your dataset is small and you want a quick representation of word counts, this works well.\n",
    "TF-IDF: More nuanced. If you want to penalize common words and give more weight to rare or important words, go for TF-IDF. For tweets, TF-IDF tends to perform better **because tweets often have a lot of common words that don't carry much meaning.\n",
    "\n",
    "*# **3. Regarding Your Training and Testing Strategy:*****\n",
    "Since you already have separate train_df for training and test_df for validation (testing), there’s no need to split your train_df. You can directly train on train_df and then test on test_df.\n",
    "\n",
    "Just ensure:\n",
    "\n",
    "You use the same vectorization process (TF-IDF or CountVectorizer) for both train_df and test_df. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733887d",
   "metadata": {
    "papermill": {
     "duration": 0.013563,
     "end_time": "2024-10-04T09:02:49.113660",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.100097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In building vectors, we will implement **TF-IDF** for text and **CountVectorizer** for keywords.\n",
    "> *Why? Go to the explication above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c5eae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:49.144398Z",
     "iopub.status.busy": "2024-10-04T09:02:49.143505Z",
     "iopub.status.idle": "2024-10-04T09:02:49.584948Z",
     "shell.execute_reply": "2024-10-04T09:02:49.583839Z"
    },
    "papermill": {
     "duration": 0.459843,
     "end_time": "2024-10-04T09:02:49.587843",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.128000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#1-CountVectorizer on keyword column\n",
    "\n",
    "#we initialize the CountVectorizer (create an object) CountVectorizer(max_features=5000)\n",
    "count_vect=CountVectorizer(max_features=100000) #very important te see the doc about max_features\n",
    "\n",
    "#fit and transform the training & test data \n",
    "train_vect_count=count_vect.fit_transform(train_df['keyword'])\n",
    "test_vect_count=count_vect.transform(test_df['keyword'])\n",
    "\n",
    "#2-TfidfVectorizer on text column\n",
    "\n",
    "#initialization \n",
    "tfidf=TfidfVectorizer(max_features=100000)\n",
    "\n",
    "#fit and transform \n",
    "train_vect_tf=tfidf.fit_transform(train_df['text'])\n",
    "test_vect_tf=tfidf.transform(test_df['text'])\n",
    "\n",
    "#the train_vect_tf, test_vect_tf, test_vect_count, train_vect_count are in general spare matrixes \n",
    "#that's why we imported hstak\n",
    "\n",
    "#NOW, let's concatenate the two matrices horizontally to create a combined feature set\n",
    "\n",
    "train_combined = hstack([train_vect_count, train_vect_tf])\n",
    "test_combined = hstack([test_vect_count, test_vect_tf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433e878",
   "metadata": {
    "papermill": {
     "duration": 0.014373,
     "end_time": "2024-10-04T09:02:49.616293",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.601920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b03ed4",
   "metadata": {
    "papermill": {
     "duration": 0.014039,
     "end_time": "2024-10-04T09:02:49.644521",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.630482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*# 2. Modeling with RidgeClassifier**\n",
    "\n",
    "*Why RidgeClassifier?\n",
    "RidgeClassifier is a linear model that applies L2 regularization. It's similar to Logistic Regression but with a squared error loss function (which is commonly used in regression tasks).\n",
    "\n",
    "Regularization: The L2 regularization helps prevent overfitting, which is useful when you have many features (like you would with text data).\n",
    "\n",
    "Advantages:\n",
    "\n",
    "It can be efficient for high-dimensional data, like text, because of its regularization.\n",
    "Works well with sparse data, which is what you get after using CountVectorizer or TF-IDF.\n",
    "When to use: It’s a solid choice when you have a large number of features (which happens when you vectorize text) and you want a simple, interpretable model.\n",
    "\n",
    "Is RidgeClassifier a good idea?\n",
    "\n",
    "Yes, it can be! RidgeClassifier works well with text data and tends to perform similarly to Logistic Regression with L2 regularization, which is often a top choice for text classification tasks like yours.\n",
    "If you are looking for a simple, efficient model that can handle a large number of features (as generated by TF-IDF or CountVectorizer), RidgeClassifier is a strong candidate.\n",
    "\n",
    "Alternatives:\n",
    "\n",
    "Logistic Regression: Also good with text data and often used for classification. You might want to compare its performance with RidgeClassifier.\n",
    "Naive Bayes (MultinomialNB): This is another common model for text classification. It can be faster than linear models and works well with both TF-IDF and CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eebf6277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:49.675686Z",
     "iopub.status.busy": "2024-10-04T09:02:49.675201Z",
     "iopub.status.idle": "2024-10-04T09:02:49.774864Z",
     "shell.execute_reply": "2024-10-04T09:02:49.773419Z"
    },
    "papermill": {
     "duration": 0.119272,
     "end_time": "2024-10-04T09:02:49.778294",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.659022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeClassifier(class_weight=&#x27;balanced&#x27;, solver=&#x27;lsqr&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeClassifier</label><div class=\"sk-toggleable__content\"><pre>RidgeClassifier(class_weight=&#x27;balanced&#x27;, solver=&#x27;lsqr&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeClassifier(class_weight='balanced', solver='lsqr')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "clf = linear_model.RidgeClassifier(class_weight='balanced',solver='lsqr',)\n",
    "clf.fit(train_combined, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e430c7",
   "metadata": {
    "papermill": {
     "duration": 0.046033,
     "end_time": "2024-10-04T09:02:49.855463",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.809430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***REMARK:***\n",
    "\n",
    "if we use the model intuitivelly like this clf.fit(train_combined, Y) , we will get an error \"cg() got an unexpected keyword argument 'tol') which is related to an incompatibility between the RidgeClassifier and the version of scipy we are using in the kaggle environment\".\n",
    "\n",
    "So, we must: \n",
    "\n",
    "1. upgrade scipy (not possible on this env) \n",
    "2. Use a different solver: The default solver for RidgeClassifier when working with sparse matrices is sparse_cg, which is causing this error. You can change the solver to lsqr, which works better with sparse data without triggering this error. clf = RidgeClassifier(solver='lsqr')\n",
    "3. Convert the sparse matrix to a dense matrix only if the data set is not too large but this is not scalabl solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b789d",
   "metadata": {
    "papermill": {
     "duration": 0.014897,
     "end_time": "2024-10-04T09:02:49.901350",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.886453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faae6c1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:49.934683Z",
     "iopub.status.busy": "2024-10-04T09:02:49.933773Z",
     "iopub.status.idle": "2024-10-04T09:02:50.381713Z",
     "shell.execute_reply": "2024-10-04T09:02:50.380095Z"
    },
    "papermill": {
     "duration": 0.471064,
     "end_time": "2024-10-04T09:02:50.386619",
     "exception": false,
     "start_time": "2024-10-04T09:02:49.915555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy scores: [0.69402495 0.53775443 0.60801051 0.62812089 0.66228647]\n",
      "the mean accuracy:  0.6260394494233406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores  = cross_val_score(clf, train_combined, Y, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"the mean accuracy: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf13c27",
   "metadata": {
    "papermill": {
     "duration": 0.028837,
     "end_time": "2024-10-04T09:02:50.444672",
     "exception": false,
     "start_time": "2024-10-04T09:02:50.415835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Testing and evaluating different models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a6f70",
   "metadata": {
    "papermill": {
     "duration": 0.024206,
     "end_time": "2024-10-04T09:02:50.498178",
     "exception": false,
     "start_time": "2024-10-04T09:02:50.473972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will test four different models: Logistic Regression, RidgeClassifier, Naive Bayes (MultinomialNB), Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d304fa4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:02:50.528862Z",
     "iopub.status.busy": "2024-10-04T09:02:50.528425Z",
     "iopub.status.idle": "2024-10-04T09:04:40.001248Z",
     "shell.execute_reply": "2024-10-04T09:04:39.999788Z"
    },
    "papermill": {
     "duration": 109.510086,
     "end_time": "2024-10-04T09:04:40.022625",
     "exception": false,
     "start_time": "2024-10-04T09:02:50.512539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Cross-validation accuracy scores: [0.69468155 0.55022981 0.62376888 0.64520368 0.68068331]\n",
      "Logistic Regression Mean accuracy: 0.6389134454354303\n",
      "\n",
      "Training Ridge Classifier...\n",
      "Ridge Classifier Cross-validation accuracy scores: [0.69402495 0.53775443 0.60801051 0.62812089 0.66228647]\n",
      "Ridge Classifier Mean accuracy: 0.6260394494233406\n",
      "\n",
      "Training Multinomial Naive Bayes...\n",
      "Multinomial Naive Bayes Cross-validation accuracy scores: [0.68089297 0.54826001 0.63099146 0.65637319 0.69973719]\n",
      "Multinomial Naive Bayes Mean accuracy: 0.6432509665635033\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Cross-validation accuracy scores: [0.73342088 0.6086671  0.63230466 0.65637319 0.73390276]\n",
      "Random Forest Mean accuracy: 0.6729337197574122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score #for the evaluation\n",
    "import numpy as np\n",
    "\n",
    "#List of the models initialized\n",
    "models=[\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs')),\n",
    "    ('Ridge Classifier', RidgeClassifier(class_weight='balanced', solver='lsqr')),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42))\n",
    "]\n",
    "\n",
    "# List to store the cross-validation scores\n",
    "model_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models:\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(model, train_combined, Y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Store the scores in the dictionary\n",
    "    model_scores[name] = scores\n",
    "    print(f\"{name} Cross-validation accuracy scores: {scores}\")\n",
    "    print(f\"{name} Mean accuracy: {np.mean(scores)}\\n\")\n",
    "\n",
    "#let's select the best model \n",
    "best_model_name = max(model_scores, key=lambda name: np.mean(model_scores[name]))\n",
    "best_model= None\n",
    "for name, model in models: \n",
    "    if name==best_model_name: \n",
    "        best_model=model\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe7ac4",
   "metadata": {
    "papermill": {
     "duration": 0.014586,
     "end_time": "2024-10-04T09:04:40.051980",
     "exception": false,
     "start_time": "2024-10-04T09:04:40.037394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> **NOTE: ****From this comparision we notice that the best model is Random Forest with a Mean_accuracy=0.67**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522fa40",
   "metadata": {
    "papermill": {
     "duration": 0.014421,
     "end_time": "2024-10-04T09:04:40.080896",
     "exception": false,
     "start_time": "2024-10-04T09:04:40.066475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Graphical comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6f006e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T09:04:40.112725Z",
     "iopub.status.busy": "2024-10-04T09:04:40.112262Z",
     "iopub.status.idle": "2024-10-04T09:04:40.632735Z",
     "shell.execute_reply": "2024-10-04T09:04:40.631126Z"
    },
    "papermill": {
     "duration": 0.539476,
     "end_time": "2024-10-04T09:04:40.634979",
     "exception": true,
     "start_time": "2024-10-04T09:04:40.095503",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'linebar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a bar plot\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinebar\u001b[49m(model_names, mean_scores, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Comparison Based on Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'linebar'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extracting model names and their mean accuracy scores into x and y\n",
    "model_names = list(model_scores.keys())\n",
    "mean_scores = [np.mean(scores) for scores in model_scores.values()]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.linebar(model_names, mean_scores, color='green')\n",
    "plt.xlabel('Mean Accuracy')\n",
    "plt.title('Model Comparison Based on Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b405002",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-04T08:05:15.385725Z",
     "iopub.status.idle": "2024-10-04T08:05:15.386280Z",
     "shell.execute_reply": "2024-10-04T08:05:15.386065Z",
     "shell.execute_reply.started": "2024-10-04T08:05:15.385992Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a box plot for cross-validation scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(model_scores.values(), labels=model_scores.keys())\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Accuracy Distribution for Each Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f68bc4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 8. Continu the prediction with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475faa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T08:28:15.655046Z",
     "iopub.status.busy": "2024-10-04T08:28:15.654239Z",
     "iopub.status.idle": "2024-10-04T08:28:35.364788Z",
     "shell.execute_reply": "2024-10-04T08:28:35.363229Z",
     "shell.execute_reply.started": "2024-10-04T08:28:15.654999Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#fiting the model \n",
    "best_model.fit(train_combined, Y )\n",
    "#makeing prediction\n",
    "Y_pred=best_model.predict(test_combined)\n",
    "\n",
    "#we don't have the Y target we are supposed to find because it's a compitition so the correction result or the score will be knowen at the result \n",
    "\n",
    "\n",
    "#Visualization of the predicted labels where we will visualize how many tweets are predicted as real disaster tweets vs. non desaster.\n",
    "unique, count = np.unique(Y_pred, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(unique, count, color=[\"green\", \"red\"])\n",
    "plt.xticks([0, 1], ['Non-Disaster', 'Disaster'])\n",
    "plt.ylabel('Number of predictions')\n",
    "plt.title(\"Distribution of predicted labels (Non-Disaster vs. Disaster)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7a484",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 9. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fd4d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T08:56:33.447869Z",
     "iopub.status.busy": "2024-10-04T08:56:33.447256Z",
     "iopub.status.idle": "2024-10-04T08:56:33.464189Z",
     "shell.execute_reply": "2024-10-04T08:56:33.462845Z",
     "shell.execute_reply.started": "2024-10-04T08:56:33.447813Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = Y_pred\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    },
    {
     "datasetId": 5815331,
     "sourceId": 9545412,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 119.862467,
   "end_time": "2024-10-04T09:04:41.373278",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T09:02:41.510811",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
